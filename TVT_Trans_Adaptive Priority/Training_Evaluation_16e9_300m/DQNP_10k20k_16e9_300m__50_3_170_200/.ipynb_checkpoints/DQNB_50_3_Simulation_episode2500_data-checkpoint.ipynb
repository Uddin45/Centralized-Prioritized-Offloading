{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_2c3s5mPjSr"
   },
   "source": [
    "+++++[link text](https:// [link text](https://))### 1. Read environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jc1n8GcSNWxH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1117,
     "status": "ok",
     "timestamp": 1705362321766,
     "user": {
      "displayName": "Ashab Uddin",
      "userId": "01269056925752111948"
     },
     "user_tz": 300
    },
    "id": "4v2mf8TUrIW6",
    "outputId": "edd8159b-c99d-48b5-bcfc-37a584fe662b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory: C:\\Users\\uddin81\\Documents\\Customize_RL_MEC Env\\Test for Journal\\update_nocondition\\new_weight\\Sever_Frequency\\for Update Graph\\10k20k_16e9_300m__50_3_170_200\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current Directory:\", current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pw1W2pj5R-9H"
   },
   "outputs": [],
   "source": [
    "# ! pip install fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2439,
     "status": "ok",
     "timestamp": 1705362324095,
     "user": {
      "displayName": "Ashab Uddin",
      "userId": "01269056925752111948"
     },
     "user_tz": 300
    },
    "id": "yDgZsDSYPjSu",
    "outputId": "33fb275d-ff33-4095-d717-846af1fc4bc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n",
      "{}\n",
      "104\n",
      "80\n",
      "20\n",
      "{'data_size': 15566, 'circle': 8511092, 'user_index': 45, 'priority': 1, 'user_distance': [466.9634973884711, 167.1558006109404, 133.5185959426485], 'delta_max': 0.197}\n",
      "{'data_size': 11236, 'circle': 8237068, 'user_index': 42, 'priority': 2, 'user_distance': [31.98085084510564, 330.5285164142056, 630.456521502613], 'delta_max': 0.181}\n",
      "{'data_size': 15416, 'circle': 9397824, 'user_index': 49, 'priority': 3, 'user_distance': [217.77230247660535, 83.06157656642479, 382.58812746534363], 'delta_max': 0.17}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from mec_environment import MECEnvironment\n",
    "from random_agent import RandomAgent\n",
    "\n",
    "mec_evn = MECEnvironment(user_cnt=50, mec_cnt=3,Env_Type=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALZLc1DEPjSw"
   },
   "source": [
    "### 2. Examine the State and Action Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1705362324096,
     "user": {
      "displayName": "Ashab Uddin",
      "userId": "01269056925752111948"
     },
     "user_tz": 300
    },
    "id": "tLC_64NIPjSx",
    "outputId": "449b79d3-3ec3-41eb-e40f-0841df76c732",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "Action size: 15\n",
      "Action space: [(0, 0.1), (0, 0.325), (0, 0.55), (0, 0.775), (0, 1.0), (1, 0.1), (1, 0.325), (1, 0.55), (1, 0.775), (1, 1.0), (2, 0.1), (2, 0.325), (2, 0.55), (2, 0.775), (2, 1.0)]\n",
      "Observes a state with length: 13\n",
      "state size: 13\n",
      "The state for the first agent looks like: [4.36529250e-07 2.62227314e-02 9.99656125e-01 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 1.44250000e-01 8.96119600e+01 1.84000000e-01\n",
      " 1.00000000e+00]\n",
      "max_freq [0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "action_size = mec_evn.action_size()\n",
    "\n",
    "task=mec_evn.get_task()\n",
    "\n",
    "states = mec_evn.get_state(task)\n",
    "print(mec_evn.trans_speed_matrix)\n",
    "state_size = mec_evn.state_size()\n",
    "action_space = mec_evn.actions\n",
    "print('Action size: {}'.format(action_size))\n",
    "print('Action space: {}'.format(action_space))\n",
    "print('Observes a state with length: {}'.format(state_size))\n",
    "print('state size: {}'.format(len(states)))\n",
    "print('The state for the first agent looks like:', states)\n",
    "print('max_freq',mec_evn.max_freq_server)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vapCU_9PjSy"
   },
   "source": [
    "### 3. Take Random Actions in the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OF7ASsRcPjSz"
   },
   "source": [
    "### 6. Take Actions with DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dq2lQI0LPjSz"
   },
   "outputs": [],
   "source": [
    "epoch_no =3000\n",
    "window_size =250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "OmtnPud5PjS0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import deque\n",
    "from greedy_agent import GreedyAgent\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "PSLBUj5oPjS1"
   },
   "outputs": [],
   "source": [
    "from dqn_agent import Agent\n",
    "agent = Agent(state_size=state_size,action_size=action_size,seed=0)\n",
    "model_path = os.path.join(current_directory, 'model_dqnb_checkpoint.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "l95PPTa7cymG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(10, 45.08324094593227), (45.08324094593227, 80.16648189186454), (80.16648189186454, 115.24972283779681), (115.24972283779681, 150.33296378372907), (150.33296378372907, inf)]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "Mec_2_Mec=300\n",
    "Range=(Mec_2_Mec/2)\n",
    "\n",
    "max_dis=math.sqrt(10*10+Range*Range)\n",
    "R_1=(max_dis-10)/4\n",
    "\n",
    "\n",
    "bucket_ranges = [(10, 10+R_1) ,(10+R_1,10+2*R_1),(10+2*R_1,10+3*R_1),(10+3*R_1,10+4*R_1),(10+4*R_1,float('inf'))]\n",
    "\n",
    "print(bucket_ranges)\n",
    "# Function to determine the bucket for a given value\n",
    "def find_bucket(value, ranges):\n",
    "    for start, end in ranges:\n",
    "        if start <= value <= end:\n",
    "            return (start, end)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zjCv3Ed6cymG",
    "outputId": "bcea003c-9348-4f67-8de6-25de32081dc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 250\tAverage Score: 236.8364\tmeantaskP1: 0.5918\tmeantaskP2: 0.5764\tmeantaskP3: 0.5000\tmeanenergyP1:     3.2860\tmeanenergyP2: 11.5551\tmeanenergyP3: 198.4874 \tmeanlatencyP1: 0.0526 \tmeanlatencyP2: 0.0536 \tmeanlatencyP3:     0.0458 \tmeandatasize: 14980.4878 \tmeandatasizeP1: 14709.8793 \tmeandatasizeP2: 15339.2350 \tmeandatasizeP3: 14875.2885 \tmean_tran_latencyP1: 0.0398      \tmean_tran_latencyP2: 0.0404  \tmean_tran_latencyP3: 0.0331  \tmean_comp_latencyP1: 0.0129      \tmean_comp_latencyP2: 0.0132 \tmean_comp_latencyP3: 0.0127\n",
      "mean_task_till_this_window 0.449052 T1:: 0.4557604992771148 T2:: 0.4435289532651837 T3:: 0.4377206436332859\n",
      "task_select_P1 499.784 task_select_P2 399.764 task_select_P3 100.452\n",
      "total_task_done_P1:: 227.788 total_task_done_P2:: 177.376 total_task_done_P3:: 43.888\n",
      "mean_task_P1_ratio_by_bucket_51_10_100 0.9931934386079645 0.9106101775401348 0.6675280990937255 0.3425713415501898 0.017639341271293418\n",
      "mean_task_P2_ratio_by_bucket_51_10_100 0.993299813607498 0.9015464982691115 0.6490241851094558 0.31597759898193883 0.015296318840787236\n",
      "mean_task_P3_ratio_by_bucket_51_10_100 0.9942049682880928 0.896920096063478 0.6193279291099711 0.3105174525680718 0.011317438889924942\n",
      "mean_task_P1_select_by_bucket_51_10_100 91.128 73.076 69.696 60.792 205.092\n",
      "mean_task_P2_select_by_bucket_51_10_100 71.588 57.656 55.76 49.348 165.412\n",
      "mean_task_P3_select_by_bucket_51_10_100 17.972 14.592 14.152 12.116 41.62\n",
      "mean_task_P1_latency_by_bucket_51_10_100 0.005091286717710444 0.03486382711784998 0.07725201129621055 0.1116482283332561 0.1285715362156717\n",
      "mean_task_P2_latency_by_bucket_51_10_100 0.004887615086618376 0.034210003004369244 0.07405348332668672 0.10700618303483457 0.1157989104750735\n",
      "mean_task_P3_latency_by_bucket_51_10_100 0.004940284984478641 0.03200398425475656 0.07219316539712638 0.1010626477688112 0.04575437616356638\n",
      "mean_task_P1_energy_by_bucket_51_10_100 1.3920435494785803 1.4635357805767206 1.5499703329193704 2.077031990045156 50.85865859333582\n",
      "mean_task_P2_energy_by_bucket_51_10_100 1.4038585285177878 1.4174067955067366 1.6077058493647942 1.841888797198731 181.06543553949786\n",
      "mean_task_P3_energy_by_bucket_51_10_100 1.4004648673872706 1.4973193571588441 1.7008644503667363 1.811311603280954 70.5357964101957\n",
      "mean_data_P1_data_by_bucket_51_10_100 14975.615631067521 14921.183758502677 14759.51315600038 14357.43748199831 12939.184814285714\n",
      "mean_data_P2_data_by_bucket_51_10_100 14997.167365755644 14928.512939311058 14742.65933785351 14278.658062260392 11795.432014285714\n",
      "mean_data_P3_data_by_bucket_51_10_100 14898.288925628407 15002.513498070874 14665.997716452948 14088.610246031747 4966.356\n",
      "all_tran 175.9396362596526 all_comp 0.011455124285769058\n",
      "all_tran_P1 0.04026927116986464 all_tran_P2 0.038232562762789385 all_tran_P3 0.03608306208065486\n",
      "all_comp_P1 0.011926779912829524 all_comp_P2 0.011907242956167234 all_comp_P3 0.011931221532640004\n",
      "all_data_P1 14990.905896409373 all_data_P2 15007.361039612471 all_data_P3 14998.339768889387\n",
      "epi_comp_energy 1.9653312758039247\n",
      "epi_comp_freq 3457172400.0\n",
      "epi_comp_latency 0.011455124285769058\n",
      "Episode 500\tAverage Score: 268.0982\tmeantaskP1: 0.6232\tmeantaskP2: 0.5945\tmeantaskP3: 0.5675\tmeanenergyP1:     3.2634\tmeanenergyP2: 9.4214\tmeanenergyP3: 150.9178 \tmeanlatencyP1: 0.0522 \tmeanlatencyP2: 0.0522 \tmeanlatencyP3:     0.0468 \tmeandatasize: 14937.8573 \tmeandatasizeP1: 14703.5817 \tmeandatasizeP2: 15233.9841 \tmeandatasizeP3: 14930.3022 \tmean_tran_latencyP1: 0.0393      \tmean_tran_latencyP2: 0.0390  \tmean_tran_latencyP3: 0.0339  \tmean_comp_latencyP1: 0.0129      \tmean_comp_latencyP2: 0.0132 \tmean_comp_latencyP3: 0.0129\n",
      "mean_task_till_this_window 0.646636 T1:: 0.6518330683323066 T2:: 0.6426486483941126 T3:: 0.6356669419226688\n",
      "task_select_P1 499.372 task_select_P2 401.364 task_select_P3 99.264\n",
      "total_task_done_P1:: 325.496 total_task_done_P2:: 258.004 total_task_done_P3:: 63.136\n",
      "mean_task_P1_ratio_by_bucket_51_10_100 0.9933931034844882 0.9124532643780681 0.6799891456080968 0.3559389141997697 0.05129240205247548\n",
      "mean_task_P2_ratio_by_bucket_51_10_100 0.9933541065691441 0.9082230125737436 0.6635670490277096 0.3308189939752308 0.043514217739995996\n",
      "mean_task_P3_ratio_by_bucket_51_10_100 0.9930499640934111 0.9064118847380344 0.6479854801089939 0.32586995909142624 0.03863752681562616\n",
      "mean_task_P1_select_by_bucket_51_10_100 130.016 105.392 98.84 81.112 84.012\n",
      "mean_task_P2_select_by_bucket_51_10_100 104.856 85.04 78.568 65.392 67.508\n",
      "mean_task_P3_select_by_bucket_51_10_100 26.072 20.456 19.612 16.308 16.816\n",
      "mean_task_P1_latency_by_bucket_51_10_100 0.005000145225077265 0.03485644547354163 0.07614988658902663 0.1106926288292687 0.13130710336649093\n",
      "mean_task_P2_latency_by_bucket_51_10_100 0.004989434285924467 0.03379208689739228 0.07351452568710169 0.10600904875120565 0.12036507361015883\n",
      "mean_task_P3_latency_by_bucket_51_10_100 0.0050443020010843016 0.03326244888906146 0.07032289607350944 0.0980689818880726 0.054329392677933935\n",
      "mean_task_P1_energy_by_bucket_51_10_100 0.5895947460654987 0.6165548681389198 0.6876586202499267 0.8427870996283506 112.31748682982004\n",
      "mean_task_P2_energy_by_bucket_51_10_100 0.5889196508844677 0.5870933172990516 0.6710739568218346 0.9738185982160019 47.820031200504395\n",
      "mean_task_P3_energy_by_bucket_51_10_100 0.5999078525905399 0.6103318127561468 0.7159660955742831 0.8498547747078491 86.88311534906258\n",
      "mean_data_P1_data_by_bucket_51_10_100 15008.243958812896 14951.761609672132 14743.905956662713 14367.699439439159 13479.150084348985\n",
      "mean_data_P2_data_by_bucket_51_10_100 15005.542052665627 14905.243803351477 14729.623618861953 14306.281835675574 12702.593314285714\n",
      "mean_data_P3_data_by_bucket_51_10_100 14989.416935107112 14937.675560784859 14725.872117026995 13993.202357819957 5993.189\n",
      "all_tran 43.08135861329545 all_comp 0.013050534440489274\n",
      "all_tran_P1 0.03951322450345796 all_tran_P2 0.03730548392735199 all_tran_P3 0.035579489053432826\n",
      "all_comp_P1 0.013260478544546657 all_comp_P2 0.013259612691099053 all_comp_P3 0.013222538016176075\n",
      "all_data_P1 15009.038746253787 all_data_P2 15001.125634007345 all_data_P3 14967.678623095762\n",
      "epi_comp_energy 0.8677752597511464\n",
      "epi_comp_freq 2498702000.0\n",
      "epi_comp_latency 0.013050534440489274\n",
      "Episode 750\tAverage Score: 276.9954\tmeantaskP1: 0.6403\tmeantaskP2: 0.6107\tmeantaskP3: 0.5894\tmeanenergyP1:     3.0379\tmeanenergyP2: 7.9840\tmeanenergyP3: 123.6456 \tmeanlatencyP1: 0.0520 \tmeanlatencyP2: 0.0521 \tmeanlatencyP3:     0.0468 \tmeandatasize: 14913.8192 \tmeandatasizeP1: 14718.9636 \tmeandatasizeP2: 15155.1342 \tmeandatasizeP3: 14934.3190 \tmean_tran_latencyP1: 0.0391      \tmean_tran_latencyP2: 0.0389  \tmean_tran_latencyP3: 0.0339  \tmean_comp_latencyP1: 0.0129      \tmean_comp_latencyP2: 0.0132 \tmean_comp_latencyP3: 0.0129\n",
      "mean_task_till_this_window 0.7015039999999999 T1:: 0.7066076143109717 T2:: 0.6992352073752381 T3:: 0.6848311155657132\n",
      "task_select_P1 500.72 task_select_P2 398.268 task_select_P3 101.012\n",
      "total_task_done_P1:: 353.808 total_task_done_P2:: 278.5 total_task_done_P3:: 69.196\n",
      "mean_task_P1_ratio_by_bucket_51_10_100 0.994122267916996 0.9128357374414683 0.6776960766994805 0.358986828930662 0.09084242546915706\n",
      "mean_task_P2_ratio_by_bucket_51_10_100 0.9927359943726001 0.9092862767075558 0.6678459036101404 0.33753711924136437 0.07853989210057757\n",
      "mean_task_P3_ratio_by_bucket_51_10_100 0.9926161626641019 0.9071174218958863 0.6390045484854475 0.31423691348683336 0.07386455912508544\n",
      "mean_task_P1_select_by_bucket_51_10_100 142.164 114.028 106.832 88.02 49.676\n",
      "mean_task_P2_select_by_bucket_51_10_100 112.592 91.728 84.732 70.212 39.004\n",
      "mean_task_P3_select_by_bucket_51_10_100 28.484 22.992 21.504 17.764 10.268\n",
      "mean_task_P1_latency_by_bucket_51_10_100 0.005009344640476228 0.03482317427137931 0.0762074548068978 0.11013802746238613 0.13435272223306532\n",
      "mean_task_P2_latency_by_bucket_51_10_100 0.005010170126730778 0.033882310475428085 0.07354442492079127 0.10659570977155897 0.11947903613873924\n",
      "mean_task_P3_latency_by_bucket_51_10_100 0.004689332012103255 0.03319610252497442 0.0705402724920018 0.10065007343779496 0.061888063557975145\n",
      "mean_task_P1_energy_by_bucket_51_10_100 0.4604009954846839 0.4711239156809212 0.5401351024065474 0.6801158895323847 17.43572609518893\n",
      "mean_task_P2_energy_by_bucket_51_10_100 0.4577552555582379 0.4783975396955365 0.5539414860901853 0.6587024106354108 17.14631776279324\n",
      "mean_task_P3_energy_by_bucket_51_10_100 0.4456647769165735 0.45989726799054337 0.5338489655716699 0.6732384086600585 16.082965348038982\n",
      "mean_data_P1_data_by_bucket_51_10_100 15007.204696564208 14970.003509744216 14766.75494718407 14386.060602377896 13855.391069119769\n",
      "mean_data_P2_data_by_bucket_51_10_100 14978.341948976997 14964.528549444705 14747.944419813834 14402.368875110707 12818.994085714286\n",
      "mean_data_P3_data_by_bucket_51_10_100 14965.46123390286 14914.746189723212 14680.567865484643 14144.58266772117 6910.455666666668\n",
      "all_tran 5.675588286182556 all_comp 0.013388833639219043\n",
      "all_tran_P1 0.039355902892814126 all_tran_P2 0.037524287972310354 all_tran_P3 0.03541479420434086\n",
      "all_comp_P1 0.013422313085055357 all_comp_P2 0.013436055783447646 all_comp_P3 0.01335718897105953\n",
      "all_data_P1 15012.350964878075 all_data_P2 15005.243963600426 all_data_P3 14976.560994533365\n",
      "epi_comp_energy 0.5403850523748289\n",
      "epi_comp_freq 2212031600.0\n",
      "epi_comp_latency 0.013388833639219043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000\tAverage Score: 279.5666\tmeantaskP1: 0.6529\tmeantaskP2: 0.6242\tmeantaskP3: 0.5989\tmeanenergyP1:     2.8637\tmeanenergyP2: 6.9911\tmeanenergyP3: 105.7190 \tmeanlatencyP1: 0.0522 \tmeanlatencyP2: 0.0523 \tmeanlatencyP3:     0.0474 \tmeandatasize: 14900.8941 \tmeandatasizeP1: 14741.4227 \tmeandatasizeP2: 15101.4688 \tmeandatasizeP3: 14899.9639 \tmean_tran_latencyP1: 0.0393      \tmean_tran_latencyP2: 0.0391  \tmean_tran_latencyP3: 0.0345  \tmean_comp_latencyP1: 0.0129      \tmean_comp_latencyP2: 0.0132 \tmean_comp_latencyP3: 0.0129\n",
      "mean_task_till_this_window 0.7176599999999999 T1:: 0.723938716930781 T2:: 0.7127453715789381 T3:: 0.7059557338708154\n",
      "task_select_P1 500.924 task_select_P2 399.196 task_select_P3 99.88\n",
      "total_task_done_P1:: 362.652 total_task_done_P2:: 284.548 total_task_done_P3:: 70.46\n",
      "mean_task_P1_ratio_by_bucket_51_10_100 0.9930493105267002 0.9108542459103799 0.6779937919758321 0.3618377321373215 0.11280545551524424\n",
      "mean_task_P2_ratio_by_bucket_51_10_100 0.9936314880274649 0.9075288483444829 0.6644563860054651 0.34003712800746383 0.0980502105247428\n",
      "mean_task_P3_ratio_by_bucket_51_10_100 0.9933214276296255 0.9123167958616841 0.6485012519672061 0.3173339702708868 0.09203940503940504\n",
      "mean_task_P1_select_by_bucket_51_10_100 146.108 117.344 109.248 89.02 39.204\n",
      "mean_task_P2_select_by_bucket_51_10_100 114.992 93.288 87.592 71.648 31.676\n",
      "mean_task_P3_select_by_bucket_51_10_100 28.696 23.764 21.42 18.024 7.976\n",
      "mean_task_P1_latency_by_bucket_51_10_100 0.005006593831513447 0.035174010174236985 0.07517698344280627 0.11042973321761042 0.1336332893523462\n",
      "mean_task_P2_latency_by_bucket_51_10_100 0.004859708963771256 0.03370521306801203 0.07329257342549224 0.10677875940752367 0.12418673554668183\n",
      "mean_task_P3_latency_by_bucket_51_10_100 0.004657654844906623 0.03336299522335032 0.07214423826676233 0.10155703963883067 0.06634306806846405\n",
      "mean_task_P1_energy_by_bucket_51_10_100 0.42349674201137977 0.442940163697882 0.502952138143532 0.6166895881963464 9.85654240398927\n",
      "mean_task_P2_energy_by_bucket_51_10_100 0.43193684225205836 0.4467646505986061 0.506132890746796 0.6216220169399072 9.422668190724446\n",
      "mean_task_P3_energy_by_bucket_51_10_100 0.41499325202100606 0.45261591154319875 0.5292137846027524 0.619979341794238 25.305582633738144\n",
      "mean_data_P1_data_by_bucket_51_10_100 15015.891555590613 14977.778523981124 14724.860371260049 14313.147113610083 13686.510309057609\n",
      "mean_data_P2_data_by_bucket_51_10_100 15012.446671816855 14919.595218250104 14742.802149407675 14339.586898914009 13166.446987301586\n",
      "mean_data_P3_data_by_bucket_51_10_100 15030.035593217644 14924.363370195832 14753.951008268397 14294.258064635364 7490.867333333334\n",
      "all_tran 3.345625716889328 all_comp 0.013555578422777182\n",
      "all_tran_P1 0.03916923268346707 all_tran_P2 0.0375064120041801 all_tran_P3 0.03591937164064416\n",
      "all_comp_P1 0.013500818246888048 all_comp_P2 0.013538643532894796 all_comp_P3 0.013515740797803634\n",
      "all_data_P1 15010.446827614247 all_data_P2 15005.571586009537 all_data_P3 15024.625961936317\n",
      "epi_comp_energy 0.4489135238050876\n",
      "epi_comp_freq 2134566400.0\n",
      "epi_comp_latency 0.013555578422777182\n",
      "Episode 1250\tAverage Score: 280.0738\tmeantaskP1: 0.6604\tmeantaskP2: 0.6344\tmeantaskP3: 0.6072\tmeanenergyP1:     2.6845\tmeanenergyP2: 6.2559\tmeanenergyP3: 92.9041 \tmeanlatencyP1: 0.0522 \tmeanlatencyP2: 0.0526 \tmeanlatencyP3:     0.0473 \tmeandatasize: 14892.6245 \tmeandatasizeP1: 14752.3132 \tmeandatasizeP2: 15069.4302 \tmeandatasizeP3: 14884.9632 \tmean_tran_latencyP1: 0.0393      \tmean_tran_latencyP2: 0.0394  \tmean_tran_latencyP3: 0.0343  \tmean_comp_latencyP1: 0.0130      \tmean_comp_latencyP2: 0.0133 \tmean_comp_latencyP3: 0.0130\n",
      "mean_task_till_this_window 0.7193160000000001 T1:: 0.7254240613490126 T2:: 0.7169377600121117 T3:: 0.6980921977766593\n",
      "task_select_P1 501.084 task_select_P2 399.484 task_select_P3 99.432\n",
      "total_task_done_P1:: 363.508 total_task_done_P2:: 286.412 total_task_done_P3:: 69.396\n",
      "mean_task_P1_ratio_by_bucket_51_10_100 0.9931992714613331 0.9118342677279613 0.6774523199073322 0.36602858181145587 0.11570214731739902\n",
      "mean_task_P2_ratio_by_bucket_51_10_100 0.9925757834477914 0.9062978708712366 0.6689088309512715 0.34017300883498847 0.1071963639918722\n",
      "mean_task_P3_ratio_by_bucket_51_10_100 0.994292340317321 0.889855900335805 0.6441394611259307 0.31686856743835473 0.08560707625707625\n",
      "mean_task_P1_select_by_bucket_51_10_100 145.504 117.232 109.94 90.932 37.476\n",
      "mean_task_P2_select_by_bucket_51_10_100 116.256 93.384 87.324 72.768 29.752\n",
      "mean_task_P3_select_by_bucket_51_10_100 28.428 23.248 21.652 18.476 7.628\n",
      "mean_task_P1_latency_by_bucket_51_10_100 0.00498445987419216 0.034432358865058625 0.07633647069393601 0.1104560694461718 0.13160550839870813\n",
      "mean_task_P2_latency_by_bucket_51_10_100 0.0049376537806411205 0.033801383313316835 0.07365029719990487 0.10622283307558854 0.1228880361551534\n",
      "mean_task_P3_latency_by_bucket_51_10_100 0.005005651730882088 0.03358875605438466 0.07047363111671837 0.10344276690078286 0.058611847224449565\n",
      "mean_task_P1_energy_by_bucket_51_10_100 0.41514559985922656 0.43848109043228517 0.4996420451762177 0.6060607245088484 2.0525158842815037\n",
      "mean_task_P2_energy_by_bucket_51_10_100 0.4167547965370844 0.43353329290503106 0.4991061019409323 0.6095960924200942 2.856900238781068\n",
      "mean_task_P3_energy_by_bucket_51_10_100 0.40876590972288535 0.43485343672992144 0.48936249561073075 0.6301676031149851 1.3639961567660908\n",
      "mean_data_P1_data_by_bucket_51_10_100 14975.906096049945 14942.403699846025 14798.07972368131 14372.76378718062 13458.868863924965\n",
      "mean_data_P2_data_by_bucket_51_10_100 15010.919778322554 14902.871855666546 14780.652166497763 14367.207823864564 12908.574142857142\n",
      "mean_data_P3_data_by_bucket_51_10_100 14979.32115885881 14903.347408906575 14759.669354290016 14382.762125541125 6564.938666666667\n",
      "all_tran 0.6475226206021323 all_comp 0.01357762473311401\n",
      "all_tran_P1 0.03945852184188753 all_tran_P2 0.03758497656494186 all_tran_P3 0.03596987185540795\n",
      "all_comp_P1 0.01352793248348559 all_comp_P2 0.013534735581425967 all_comp_P3 0.013561251373976724\n",
      "all_data_P1 15005.357267121508 all_data_P2 15004.151990020495 all_data_P3 15010.467488087088\n",
      "epi_comp_energy 0.4223564684168336\n",
      "epi_comp_freq 2111184800.0\n",
      "epi_comp_latency 0.01357762473311401\n",
      "Episode 1500\tAverage Score: 280.4534\tmeantaskP1: 0.6672\tmeantaskP2: 0.6423\tmeantaskP3: 0.6144\tmeanenergyP1:     2.5238\tmeanenergyP2: 5.6877\tmeanenergyP3: 83.2166 \tmeanlatencyP1: 0.0524 \tmeanlatencyP2: 0.0528 \tmeanlatencyP3:     0.0476 \tmeandatasize: 14887.6374 \tmeandatasizeP1: 14765.8489 \tmeandatasizeP2: 15043.0177 \tmeandatasizeP3: 14868.3519 \tmean_tran_latencyP1: 0.0394      \tmean_tran_latencyP2: 0.0395  \tmean_tran_latencyP3: 0.0346  \tmean_comp_latencyP1: 0.0130      \tmean_comp_latencyP2: 0.0133 \tmean_comp_latencyP3: 0.0130\n",
      "mean_task_till_this_window 0.723248 T1:: 0.7300043336114463 T2:: 0.7190623201273966 T3:: 0.7063036123473322\n",
      "task_select_P1 500.096 task_select_P2 398.6 task_select_P3 101.304\n",
      "total_task_done_P1:: 365.08 total_task_done_P2:: 286.596 total_task_done_P3:: 71.572\n",
      "mean_task_P1_ratio_by_bucket_51_10_100 0.9937310447111918 0.9137617321497951 0.6816526696143276 0.3642243989418172 0.12120848479557389\n",
      "mean_task_P2_ratio_by_bucket_51_10_100 0.993321762941519 0.907317122170039 0.6708286329238614 0.33724782762476907 0.10873479182162336\n",
      "mean_task_P3_ratio_by_bucket_51_10_100 0.9927020923332655 0.9006281030357728 0.641698845595357 0.30658834204196794 0.09707455322455323\n",
      "mean_task_P1_select_by_bucket_51_10_100 146.316 117.244 110.38 90.504 35.652\n",
      "mean_task_P2_select_by_bucket_51_10_100 116.712 92.644 88.228 72.208 28.808\n",
      "mean_task_P3_select_by_bucket_51_10_100 30.084 23.784 21.652 18.384 7.4\n",
      "mean_task_P1_latency_by_bucket_51_10_100 0.005137992846886421 0.0348427200608911 0.07576920817872755 0.11045669102144101 0.13242626285778517\n",
      "mean_task_P2_latency_by_bucket_51_10_100 0.00493325149179269 0.03378396069436324 0.07343701688751511 0.1063409648475492 0.12572340093324066\n",
      "mean_task_P3_latency_by_bucket_51_10_100 0.005268791602365937 0.03272515612353124 0.06997235633107789 0.10047676538574461 0.06094432111257762\n",
      "mean_task_P1_energy_by_bucket_51_10_100 0.41560543012039053 0.436188971145596 0.4942559113356639 0.6026820345399874 2.456392348475957\n",
      "mean_task_P2_energy_by_bucket_51_10_100 0.4167015892000635 0.4344058537279576 0.4927537038099289 0.6037965662192134 6.604428395742685\n",
      "mean_task_P3_energy_by_bucket_51_10_100 0.41118278899169824 0.4338600316227952 0.503304780872107 0.6215123981687429 1.1407498317927156\n",
      "mean_data_P1_data_by_bucket_51_10_100 14991.948604633322 14962.889999442588 14761.186792104334 14333.71408390188 13491.695524819625\n",
      "mean_data_P2_data_by_bucket_51_10_100 15015.540201509104 14934.178811886892 14751.017572292156 14360.888815609414 13317.380068253968\n",
      "mean_data_P3_data_by_bucket_51_10_100 15024.294610065688 14980.72558289348 14857.651350618571 14306.983002453104 7071.249733333333\n",
      "all_tran 1.0261750085685908 all_comp 0.01357673251707224\n",
      "all_tran_P1 0.03944788086919859 all_tran_P2 0.03748499803238957 all_tran_P3 0.03481123369658768\n",
      "all_comp_P1 0.013529959885184241 all_comp_P2 0.013510818545065792 all_comp_P3 0.013479019699163977\n",
      "all_data_P1 15000.554772612948 all_data_P2 15012.851838647468 all_data_P3 15040.943098041273\n",
      "epi_comp_energy 0.41385650562782\n",
      "epi_comp_freq 2106249600.0\n",
      "epi_comp_latency 0.01357673251707224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1503\tAverage Score: 280.4622\tmeantaskP1: 0.7449\tmeantaskP2: 0.6960\tmeantaskP3: 0.7753\tcomplatencyP1:             0.0136\tcomplatencyP2: 0.0135\tcomplatencyP3: 0.0133\tmeanlatencyP1: 0.0382\tmeanlatencyP2: 0.0366\tmeanlatencyP3:0.0363\tmeanlatreward:0.0431\tmeanengreward:0.0122"
     ]
    }
   ],
   "source": [
    "def dqnb(n_episodes=epoch_no, max_t=1000, eps_start=1.0, eps_end=0.001, eps_decay=0.995,bench_score=1500000):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of atimesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    epi_scores = []                        # list containing scores from each episode\n",
    "    epi_task=[]\n",
    "    epi_task_P1=[]\n",
    "    epi_task_P2=[]\n",
    "    epi_task_P3=[]\n",
    "    \n",
    "    epi_all_trans_latency=[]\n",
    "    epi_all_trans_latency_P1=[]\n",
    "    epi_all_trans_latency_P2=[]\n",
    "    epi_all_trans_latency_P3=[]\n",
    "    epi_all_comp_latency=[]\n",
    "    epi_all_comp_latency_P1=[]\n",
    "    epi_all_comp_latency_P2=[]\n",
    "    epi_all_comp_latency_P3=[]\n",
    "    \n",
    "    \n",
    "    epi_comp_latency=[]\n",
    "    epi_comp_latency_P1=[]\n",
    "    epi_comp_latency_P2=[]\n",
    "    epi_comp_latency_P3=[]\n",
    "    epi_tran_latency=[]\n",
    "    epi_tran_latency_P1=[]\n",
    "    epi_tran_latency_P2=[]\n",
    "    epi_tran_latency_P3=[]\n",
    "    epi_latency=[]\n",
    "    epi_latency_P1=[]\n",
    "    epi_latency_P2=[]\n",
    "    epi_latency_P3=[]\n",
    "    epi_task_select_P1=[]\n",
    "    epi_task_select_P2=[]\n",
    "    epi_task_select_P3=[]\n",
    "    epi_datasize=[]\n",
    "    epi_datasize_P1=[]\n",
    "    epi_datasize_P2=[]\n",
    "    epi_datasize_P3=[]\n",
    "    epi_all_datasize_P1=[]\n",
    "    epi_all_datasize_P2=[]\n",
    "    epi_all_datasize_P3=[]\n",
    "    epi_energy=[]\n",
    "    epi_energy_P1=[]\n",
    "    epi_energy_P2=[]\n",
    "    epi_energy_P3=[]\n",
    "\n",
    "    epi_num_task=[]\n",
    "    epi_num_task_P1=[]\n",
    "    epi_num_task_P2=[]\n",
    "    epi_num_task_P3=[]\n",
    "    epi_comp_energy=[]\n",
    "    epi_comp_frequency=[]\n",
    "    mean_scores = [] # average score by the windos size\n",
    "    mean_task=[] # average task ratio by window size\n",
    "    mean_energy = [] # average energy cost by window size\n",
    "    mean_latency=[]\n",
    "    mean_datasize=[]\n",
    "    mean_tran_latency=[]\n",
    "    mean_comp_latency=[]\n",
    "\n",
    "    mean_tasks_P1=[]\n",
    "    mean_tasks_P2=[]\n",
    "    mean_tasks_P3=[]\n",
    "\n",
    "    mean_energy_P1=[]\n",
    "    mean_energy_P2=[]\n",
    "    mean_energy_P3=[]\n",
    "\n",
    "    mean_latency_P1=[]\n",
    "    mean_latency_P2=[]\n",
    "    mean_latency_P3=[]\n",
    "    mean_tran_latency_P1=[]\n",
    "    mean_tran_latency_P2=[]\n",
    "    mean_tran_latency_P3=[]\n",
    "    mean_comp_latency_P1=[]\n",
    "    mean_comp_latency_P2=[]\n",
    "    mean_comp_latency_P3=[]\n",
    "\n",
    "    mean_datasize_P1=[]\n",
    "    mean_datasize_P2=[]\n",
    "    mean_datasize_P3=[]\n",
    "    reward_list_P1=[]\n",
    "    reward_list_P2=[]\n",
    "    reward_list_P3=[]\n",
    "\n",
    "    scores_window = deque(maxlen=window_size)  # last score_window_size scores\n",
    "    task_done_ratio_window = deque(maxlen=window_size)  # last  task dones\n",
    "    energy_window = deque(maxlen=window_size)  # last  engery cost\n",
    "    latency_window = deque(maxlen=window_size)  # last  engery cost\n",
    "    datasize_window= deque(maxlen=window_size)\n",
    "\n",
    "\n",
    "    mean_tasks_P1_window = deque(maxlen=window_size)  # last  engery cost\n",
    "    mean_tasks_P2_window = deque(maxlen=window_size)  # last  engery cost\n",
    "    mean_tasks_P3_window = deque(maxlen=window_size)  # last  engery cost\n",
    "\n",
    "    win_num_task = deque(maxlen=window_size)\n",
    "    win_num_task_P1 = deque(maxlen=window_size)  # last  engery cost\n",
    "    win_num_task_P2 = deque(maxlen=window_size)  # last  engery cost\n",
    "    win_num_task_P3 = deque(maxlen=window_size)  # last  engery cost\n",
    "\n",
    "    mean_datasize_window=deque(maxlen=window_size)\n",
    "    mean_datasize_P1_window= deque(maxlen=window_size)  # last  engery cost\n",
    "    mean_datasize_P2_window = deque(maxlen=window_size)  # last  engery cost\n",
    "    mean_datasize_P3_window = deque(maxlen=window_size)  # last  engery cost\n",
    "\n",
    "    mean_energy_window_P1 = deque(maxlen=window_size)  # last  engery cost\n",
    "    mean_energy_window_P2 = deque(maxlen=window_size)  # last  engery cost\n",
    "    mean_energy_window_P3 = deque(maxlen=window_size)  # last  engery cost\n",
    "\n",
    "    mean_latency_P1_window=deque(maxlen=window_size)\n",
    "    mean_latency_P2_window=deque(maxlen=window_size)\n",
    "    mean_latency_P3_window=deque(maxlen=window_size)\n",
    "    \n",
    "    mean_comp_latency_P1_window=deque(maxlen=window_size)\n",
    "    mean_comp_latency_P2_window=deque(maxlen=window_size)\n",
    "    mean_comp_latency_P3_window=deque(maxlen=window_size)\n",
    "    \n",
    "    mean_tran_latency_P1_window=deque(maxlen=window_size)\n",
    "    mean_tran_latency_P2_window=deque(maxlen=window_size)\n",
    "    mean_tran_latency_P3_window=deque(maxlen=window_size)\n",
    "        \n",
    "    \n",
    "    \n",
    "    (10, 10+R_1) ,(10+R_1,10+2*R_1),(10+2*R_1,10+3*R_1),(10+3*R_1,10+4*R_1),(10+4*R_1,float('inf'))\n",
    "    \n",
    "    bucket_tasks_mean = {(10, 10+R_1):{},(10+R_1,10+2*R_1):{},(10+2*R_1,10+3*R_1):{},\\\n",
    "                         (10+3*R_1,10+4*R_1):{},(10+4*R_1,float('inf')):{}}\n",
    "    bucket_task_len= {(10, 10+R_1):{},(10+R_1,10+2*R_1):{},(10+2*R_1,10+3*R_1):{},\\\n",
    "                         (10+3*R_1,10+4*R_1):{},(10+4*R_1,float('inf')):{}}\n",
    "    bucket_latency_mean={(10, 10+R_1):{},(10+R_1,10+2*R_1):{},(10+2*R_1,10+3*R_1):{},\\\n",
    "                         (10+3*R_1,10+4*R_1):{},(10+4*R_1,float('inf')):{}}\n",
    "    bucket_energy_mean= {(10, 10+R_1):{},(10+R_1,10+2*R_1):{},(10+2*R_1,10+3*R_1):{},\\\n",
    "                         (10+3*R_1,10+4*R_1):{},(10+4*R_1,float('inf')):{}}\n",
    "    bucket_data_mean= {(10, 10+R_1):{},(10+R_1,10+2*R_1):{},(10+2*R_1,10+3*R_1):{},\\\n",
    "                         (10+3*R_1,10+4*R_1):{},(10+4*R_1,float('inf')):{}}\n",
    "    #bucket_energy_mean={(10, 0.25*max_dis):{},(0.25*max_dis,0.50*max_dis):{},(0.5*max_dis,0.75*max_dis):{},(0.75*max_dis,1*max_dis):{},(1*max_dis,float('inf')):{}}\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        mec_evn.reset() # reset the environment\n",
    "        #mec_evn.max_freq_server= [np.random.choice([2,4,8,16])*1e9 for _ in range (3)]\n",
    "       # print(mec_evn.max_freq_server)\n",
    "     #   print(mec_evn.mec_servers)\n",
    "\n",
    "        score = 0\n",
    "        energy = []\n",
    "        latency=[]\n",
    "        task_done = []\n",
    "        energy_P1=[]\n",
    "        energy_P2=[]\n",
    "        energy_P3=[]\n",
    "        task_done_P1=[]\n",
    "        task_done_P2=[]\n",
    "        task_done_P3=[]\n",
    "        latency_P1=[]\n",
    "        latency_P2=[]\n",
    "        latency_P3=[]\n",
    "        comp_latency=[]\n",
    "        comp_latency_P1=[]\n",
    "        comp_latency_P2=[]\n",
    "        comp_latency_P3=[]\n",
    "        tran_latency=[]\n",
    "        tran_latency_P1=[]\n",
    "        tran_latency_P2=[]\n",
    "        tran_latency_P3=[]\n",
    "        compt_energy=[]\n",
    "        compt_freq=[]\n",
    "        all_trans_latency=[]\n",
    "        all_trans_latency_P1=[]\n",
    "        all_trans_latency_P2=[]\n",
    "        all_trans_latency_P3=[]\n",
    "        \n",
    "        all_comp_latency=[]\n",
    "        all_comp_latency_P1=[]\n",
    "        all_comp_latency_P2=[]\n",
    "        all_comp_latency_P3=[]\n",
    "        \n",
    "        all_datasize_P1=[]\n",
    "        all_datasize_P2=[]\n",
    "        all_datasize_P3=[]\n",
    "        datasize=[]\n",
    "        datasize_P1=[]\n",
    "        datasize_P2=[]\n",
    "        datasize_P3=[]\n",
    "\n",
    "        reward_lat=[]\n",
    "        reward_eng=[]\n",
    "        bucket_task = {range_tuple: {'P1_task_done': [], 'P2_task_done': [], 'P3_task_done': []} for range_tuple in bucket_ranges}\n",
    "        bucket_latency = {range_tuple: {'P1_latency': [], 'P2_latency': [], 'P3_latency': []} for range_tuple in bucket_ranges}\n",
    "        bucket_energy = {range_tuple: {'P1_energy': [], 'P2_energy': [], 'P3_energy': []} for range_tuple in bucket_ranges}\n",
    "        bucket_data = {range_tuple: {'P1_data': [], 'P2_data': [], 'P3_data': []} for range_tuple in bucket_ranges}\n",
    "\n",
    "\n",
    "        tasks = mec_evn.get_task()\n",
    "        state = mec_evn.get_state(tasks)\n",
    "        for t in range(max_t):\n",
    "\n",
    "            \n",
    "            action = agent.act(state, eps)\n",
    "            next_state, feedback,done,action_task,next_task,rec_mec_idx = mec_evn.step(action,tasks)\n",
    "            agent.step(state, action, feedback['reward'], next_state, done)\n",
    "           \n",
    "            #print(mec_evn.max_freq_server)\n",
    "\n",
    "            score += feedback['reward']\n",
    "            energy.append(feedback['energy'])\n",
    "            \n",
    "            user_distance=action_task['user_distance'][rec_mec_idx]\n",
    "            #print(user_distance)\n",
    "           # print(user_distance,rec_mec_idx,action_task['user_distance'])\n",
    "            bucket = find_bucket(user_distance, bucket_ranges)\n",
    "            tran_time=feedback['lat_vec'][0]\n",
    "            comp_time=feedback['lat_vec'][1]+feedback['lat_vec'][2] #comp_time+wait_time\n",
    "            reward_lat.append(feedback['reward_lat'])\n",
    "            reward_eng.append(feedback['reward_eng'])\n",
    "            all_trans_latency.append(tran_time)\n",
    "            all_comp_latency.append(comp_time)\n",
    "            trans_energy=feedback['eng_vector'][0]\n",
    "            comp_energy=feedback['eng_vector'][1]\n",
    "            compt_energy.append(comp_energy)\n",
    "            compt_freq.append(max(2*10**9,mec_evn.max_freq_server[rec_mec_idx]*mec_evn.actions[action][1]))\n",
    "            \n",
    "           # print(min(2*10**9,mec_evn.max_freq_server[rec_mec_idx]*mec_evn.actions[action][1]))\n",
    "           # print(tran_time,comp_time)\n",
    "            if feedback['task_done']==1:\n",
    "              datasize.append(action_task['data_size'])\n",
    "              latency.append(feedback['latency'])\n",
    "              comp_latency.append(comp_time)\n",
    "              tran_latency.append(tran_time)\n",
    "\n",
    "\n",
    "            if action_task['data_size'] != 0 :\n",
    "              task_done.append(feedback['task_done'])\n",
    "              \n",
    "              if action_task['priority']==1:\n",
    "                  \n",
    "                  task_done_P1.append(feedback['task_done'])\n",
    "                  energy_P1.append(feedback['energy'])\n",
    "                  \n",
    "                  \n",
    "                  bucket_task[bucket]['P1_task_done'].append(feedback['task_done'])\n",
    "                  bucket_energy[bucket]['P1_energy'].append(feedback['energy'])\n",
    "                  all_datasize_P1.append(action_task['data_size'])\n",
    "                  \n",
    "                  \n",
    "                  if feedback['task_done']:\n",
    "                    latency_P1.append(feedback['latency'])\n",
    "                    comp_latency_P1.append(comp_time)\n",
    "                    tran_latency_P1.append(tran_time)\n",
    "                    datasize_P1.append(action_task['data_size'])\n",
    "                    all_trans_latency_P1.append(tran_time)\n",
    "                    all_comp_latency_P1.append(comp_time)\n",
    "                    bucket_latency[bucket]['P1_latency'].append( tran_time)\n",
    "                    bucket_data[bucket]['P1_data'].append(action_task['data_size'])\n",
    "                  \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "\n",
    "              if action_task['priority']==2:\n",
    "                  \n",
    "                  task_done_P2.append(feedback['task_done'])\n",
    "                  energy_P2.append(feedback['energy'])\n",
    "                  \n",
    "                  \n",
    "                  bucket_task[bucket]['P2_task_done'].append(feedback['task_done'])\n",
    "                  bucket_energy[bucket]['P2_energy'].append(feedback['energy'])\n",
    "                  \n",
    "                  all_datasize_P2.append(action_task['data_size'])\n",
    "                  \n",
    "                 \n",
    "                  if feedback['task_done']:\n",
    "                    latency_P2.append(feedback['latency'])\n",
    "                    comp_latency_P2.append(comp_time)\n",
    "                    tran_latency_P2.append(tran_time)\n",
    "                    datasize_P2.append(action_task['data_size'])\n",
    "                    all_trans_latency_P2.append(tran_time)\n",
    "                    all_comp_latency_P2.append(comp_time)\n",
    "                    bucket_latency[bucket]['P2_latency'].append(tran_time)\n",
    "                    bucket_data[bucket]['P2_data'].append(action_task['data_size'])\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "\n",
    "              if action_task['priority']==3:\n",
    "                 \n",
    "                  task_done_P3.append(feedback['task_done'])\n",
    "                  energy_P3.append(feedback['energy'])\n",
    "                  \n",
    "                  \n",
    "                  bucket_task[bucket]['P3_task_done'].append(feedback['task_done'])\n",
    "                  bucket_energy[bucket]['P3_energy'].append(feedback['energy'])\n",
    "                  all_datasize_P3.append(action_task['data_size'])\n",
    "                  \n",
    "                 \n",
    "                  if feedback['task_done']:\n",
    "                    latency_P3.append(feedback['latency'])\n",
    "                    comp_latency_P3.append(comp_time)\n",
    "                    tran_latency_P3.append(tran_time)\n",
    "                    datasize_P3.append(action_task['data_size'])\n",
    "                    all_trans_latency_P3.append(tran_time)\n",
    "                    all_comp_latency_P3.append(comp_time)\n",
    "                    bucket_latency[bucket]['P3_latency'].append(tran_time)\n",
    "                    bucket_data[bucket]['P3_data'].append(action_task['data_size'])\n",
    "                    \n",
    "                    \n",
    "                   \n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "        \n",
    "                # Append values to their respective lists within the bucket\n",
    "                \n",
    "               \n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            tasks=next_task\n",
    "            state=next_state\n",
    "            if done:\n",
    "                print(t)\n",
    "                \n",
    "                break\n",
    "        \n",
    "\n",
    "        for range_tuple, tasks in bucket_task.items():\n",
    "            \n",
    "            \n",
    "            # Initialize a sub-dictionary for each range\n",
    "            for task, values in tasks.items():\n",
    "        # Calculate the mean for each task list\n",
    "                mean_value = sum(values) / len(values) if values else 0  # Ensure division is valid\n",
    "                mean_len = len(values)\n",
    "                # Create a new key for storing the mean value of the task list\n",
    "                mean_key = f'{task}_mean'\n",
    "                len_key = f'{task}_mean_len'\n",
    "                # Ensure the sub-dictionary for mean values exists for the current range_tuple\n",
    "                if mean_key not in bucket_tasks_mean[range_tuple]:\n",
    "                    bucket_tasks_mean[range_tuple][mean_key] = []\n",
    "                if len_key not in bucket_task_len[range_tuple]:\n",
    "                    bucket_task_len[range_tuple][len_key] = []\n",
    "                # Store the mean value in the corresponding range's sub-dictionary\n",
    "                bucket_tasks_mean[range_tuple][mean_key].append(mean_value)\n",
    "                bucket_task_len[range_tuple][len_key].append(mean_len)\n",
    "        for range_tuple, lat in bucket_latency.items():\n",
    "             \n",
    "            \n",
    "            # Initialize a sub-dictionary for each range\n",
    "            for l, v in lat.items():\n",
    "               # print(l,v)\n",
    "                # Calculate the mean for each task list\n",
    "                mean_value = sum(v) / len(v) if v else 0  # Ensure division is valid\n",
    "                \n",
    "                # Create a new key for storing the mean value of the task list\n",
    "                mean_key = f'{l}_mean'\n",
    "                if mean_key not in bucket_latency_mean[range_tuple]:\n",
    "                    bucket_latency_mean[range_tuple][mean_key] = []\n",
    "                \n",
    "                # Store the mean value in the corresponding range's sub-dictionary\n",
    "                bucket_latency_mean[range_tuple][mean_key].append(mean_value)\n",
    "        for range_tuple, eng in bucket_energy.items():\n",
    "             \n",
    "            \n",
    "            # Initialize a sub-dictionary for each range\n",
    "            for e, v in eng.items():\n",
    "                # Calculate the mean for each task list\n",
    "                mean_value = sum(v) / len(v) if v else 0  # Ensure division is valid\n",
    "                \n",
    "                # Create a new key for storing the mean value of the task list\n",
    "                mean_key = f'{e}_mean'\n",
    "                if mean_key not in bucket_energy_mean[range_tuple]:\n",
    "                    bucket_energy_mean[range_tuple][mean_key] = []\n",
    "                # Store the mean value in the corresponding range's sub-dictionary\n",
    "                bucket_energy_mean[range_tuple][mean_key].append(mean_value)\n",
    "        \n",
    "        for range_tuple, dat in bucket_data.items():\n",
    "             \n",
    "            \n",
    "            # Initialize a sub-dictionary for each range\n",
    "            for d, v in dat.items():\n",
    "                # Calculate the mean for each task list\n",
    "                mean_value = sum(v) / len(v) if v else 0  # Ensure division is valid\n",
    "                \n",
    "                # Create a new key for storing the mean value of the task list\n",
    "                mean_key = f'{d}_mean'\n",
    "                if mean_key not in bucket_data_mean[range_tuple]:\n",
    "                    bucket_data_mean[range_tuple][mean_key] = []\n",
    "                # Store the mean value in the corresponding range's sub-dictionary\n",
    "                bucket_data_mean[range_tuple][mean_key].append(mean_value)\n",
    "                \n",
    "\n",
    "# Now, bucket_means contains only the mean values for each task list within each bucket range.\n",
    "# Displaying the new bucket_means dictionary\n",
    "\n",
    "\n",
    "        scores_window.append(score)       # save most recent score\n",
    "        epi_scores.append(score)              # save most recent score\n",
    "        mean_score = np.mean(scores_window)\n",
    "        epi_task.append(np.mean(task_done))\n",
    "        epi_task_P1.append(np.mean(task_done_P1))\n",
    "        epi_task_P2.append(np.mean(task_done_P2))\n",
    "        epi_task_P3.append(np.mean(task_done_P3))\n",
    "\n",
    "        epi_task_select_P1.append(len(task_done_P1))\n",
    "        epi_task_select_P2.append(len(task_done_P2))\n",
    "        epi_task_select_P3.append(len(task_done_P3))\n",
    "\n",
    "        epi_latency.append(np.mean(latency))\n",
    "        epi_latency_P1.append(np.mean(latency_P1))\n",
    "        epi_latency_P2.append(np.mean(latency_P2))\n",
    "        epi_latency_P3.append(np.mean(latency_P3))\n",
    "        \n",
    "        epi_comp_latency.append(np.mean(comp_latency))\n",
    "        epi_comp_latency_P1.append(np.mean(comp_latency_P1))\n",
    "        epi_comp_latency_P2.append(np.mean(comp_latency_P2))\n",
    "        epi_comp_latency_P3.append(np.mean(comp_latency_P3))\n",
    "        \n",
    "        epi_tran_latency.append(np.mean(tran_latency))\n",
    "        epi_tran_latency_P1.append(np.mean(tran_latency_P1))\n",
    "        epi_tran_latency_P2.append(np.mean(tran_latency_P2))\n",
    "        epi_tran_latency_P3.append(np.mean(tran_latency_P3))\n",
    "        \n",
    "\n",
    "        epi_energy.append(np.mean(energy))\n",
    "        epi_energy_P1.append(np.mean(energy_P1))\n",
    "        epi_energy_P2.append(np.mean(energy_P2))\n",
    "        epi_energy_P3.append(np.mean(energy_P3))\n",
    "\n",
    "        epi_num_task.append(sum(task_done_P1)+sum(task_done_P2)+sum(task_done_P3))\n",
    "        epi_num_task_P1.append(sum(task_done_P1))\n",
    "        epi_num_task_P2.append(sum(task_done_P2))\n",
    "        epi_num_task_P3.append(sum(task_done_P3))\n",
    "\n",
    "        epi_datasize.append(np.mean(datasize))\n",
    "        epi_datasize_P1.append(np.mean(datasize_P1))\n",
    "        epi_datasize_P2.append(np.mean(datasize_P2))\n",
    "        epi_datasize_P3.append(np.mean(datasize_P3))\n",
    "        \n",
    "        epi_all_datasize_P1.append(np.mean(all_datasize_P1))\n",
    "        epi_all_datasize_P2.append(np.mean(all_datasize_P2))\n",
    "        epi_all_datasize_P3.append(np.mean(all_datasize_P3))\n",
    "\n",
    "        \n",
    "        epi_all_trans_latency.append(np.mean(all_trans_latency))\n",
    "        epi_all_trans_latency_P1.append(np.mean(all_trans_latency_P1))\n",
    "        epi_all_trans_latency_P2.append(np.mean(all_trans_latency_P2))\n",
    "        epi_all_trans_latency_P3.append(np.mean(all_trans_latency_P3))\n",
    "        epi_all_comp_latency.append(np.mean(all_comp_latency))\n",
    "        epi_all_comp_latency_P1.append(np.mean(all_comp_latency_P1))\n",
    "        epi_all_comp_latency_P2.append(np.mean(all_comp_latency_P2))\n",
    "        epi_all_comp_latency_P3.append(np.mean(all_comp_latency_P3))\n",
    "       \n",
    "        epi_comp_energy.append(np.mean(compt_energy))\n",
    "        epi_comp_frequency.append(np.mean(compt_freq))\n",
    "\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.4f}\\tmeantaskP1: {:.4f}\\tmeantaskP2: {:.4f}\\tmeantaskP3: {:.4f}\\tcomplatencyP1: \\\n",
    "            {:.4f}\\tcomplatencyP2: {:.4f}\\tcomplatencyP3: {:.4f}\\tmeanlatencyP1: {:.4f}\\tmeanlatencyP2: {:.4f}\\tmeanlatencyP3:{:.4f}\\tmeanlatreward:{:.4f}\\tmeanengreward:{:.4f}'.format(i_episode, \\\n",
    "            mean_score, np.mean(task_done_P1), np.mean(task_done_P2), np.mean(task_done_P3), np.mean(comp_latency_P1), \\\n",
    "            np.mean(comp_latency_P2), np.mean(comp_latency_P3), \\\n",
    "                                    np.mean(tran_latency_P1), np.mean(tran_latency_P2), np.mean(tran_latency_P3),np.mean(reward_lat),np.mean(reward_eng)), end=\"\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if i_episode % window_size == 0:\n",
    "            \n",
    "\n",
    "            mean_scores.append(mean_score)\n",
    "            task_done_ratio_window.append(np.mean(task_done))       # task done ratio\n",
    "            mean_task.append(np.mean(task_done_ratio_window))\n",
    "            energy_window.append(np.mean(energy))\n",
    "            latency_window.append(np.mean(latency))\n",
    "\n",
    "            mean_energy.append(np.mean(energy_window))\n",
    "            mean_latency.append(np.mean(latency_window))\n",
    "\n",
    "\n",
    "            mean_datasize_window.append(np.mean(datasize))\n",
    "            mean_datasize.append(np.mean(mean_datasize_window))\n",
    "\n",
    "            mean_tasks_P1_window.append(np.mean(task_done_P1))\n",
    "            mean_tasks_P1.append(np.mean( mean_tasks_P1_window))\n",
    "\n",
    "            mean_tasks_P2_window.append(np.mean(task_done_P2))\n",
    "            mean_tasks_P2.append(np.mean( mean_tasks_P2_window))\n",
    "\n",
    "            mean_tasks_P3_window.append(np.mean(task_done_P3))\n",
    "            mean_tasks_P3.append(np.mean( mean_tasks_P3_window))\n",
    "\n",
    "            mean_energy_window_P1.append(np.mean(energy_P1))\n",
    "            mean_energy_P1.append(np.mean(mean_energy_window_P1))\n",
    "\n",
    "            mean_energy_window_P2.append(np.mean(energy_P2))\n",
    "            mean_energy_P2.append(np.mean(mean_energy_window_P2))\n",
    "\n",
    "            mean_energy_window_P3.append(np.mean(energy_P3))\n",
    "            mean_energy_P3.append(np.mean(mean_energy_window_P3))\n",
    "\n",
    "\n",
    "            mean_latency_P1_window.append(np.mean(latency_P1))\n",
    "            mean_latency_P1.append(np.mean( mean_latency_P1_window))\n",
    "\n",
    "            mean_latency_P2_window.append(np.mean(latency_P2))\n",
    "            mean_latency_P2.append(np.mean( mean_latency_P2_window))\n",
    "\n",
    "            mean_latency_P3_window.append(np.mean(latency_P3))\n",
    "            mean_latency_P3.append(np.mean( mean_latency_P3_window))\n",
    "            \n",
    "            mean_comp_latency_P1_window.append(np.mean(comp_latency_P1))\n",
    "            mean_comp_latency_P1.append(np.mean( mean_comp_latency_P1_window))\n",
    "            mean_comp_latency_P2_window.append(np.mean(comp_latency_P2))\n",
    "            mean_comp_latency_P2.append(np.mean( mean_comp_latency_P2_window))\n",
    "            mean_comp_latency_P3_window.append(np.mean(comp_latency_P3))\n",
    "            mean_comp_latency_P3.append(np.mean( mean_comp_latency_P3_window))\n",
    "            \n",
    "            mean_tran_latency_P1_window.append(np.mean(tran_latency_P1))\n",
    "            mean_tran_latency_P1.append(np.mean( mean_tran_latency_P1_window))\n",
    "            mean_tran_latency_P2_window.append(np.mean(tran_latency_P2))\n",
    "            mean_tran_latency_P2.append(np.mean( mean_tran_latency_P2_window))\n",
    "            mean_tran_latency_P3_window.append(np.mean(tran_latency_P3))\n",
    "            mean_tran_latency_P3.append(np.mean( mean_tran_latency_P3_window))\n",
    "\n",
    "            mean_datasize_P1_window.append(np.mean(datasize_P1))\n",
    "            mean_datasize_P1.append(np.mean(mean_datasize_P1_window))\n",
    "\n",
    "            mean_datasize_P2_window.append(np.mean(datasize_P2))\n",
    "            mean_datasize_P2.append(np.mean(mean_datasize_P2_window))\n",
    "\n",
    "            mean_datasize_P3_window.append(np.mean(datasize_P3))\n",
    "            mean_datasize_P3.append(np.mean(mean_datasize_P3_window))\n",
    "\n",
    "\n",
    "            win_num_task.append(sum(task_done_P1)+sum(task_done_P2)+sum(task_done_P3))\n",
    "            win_num_task_P1.append(sum(task_done_P1))\n",
    "            win_num_task_P2.append(sum(task_done_P2))\n",
    "            win_num_task_P3.append(sum(task_done_P3))\n",
    "\n",
    "\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.4f}\\tmeantaskP1: {:.4f}\\tmeantaskP2: {:.4f}\\tmeantaskP3: {:.4f}\\tmeanenergyP1: \\\n",
    "    {:.4f}\\tmeanenergyP2: {:.4f}\\tmeanenergyP3: {:.4f} \\tmeanlatencyP1: {:.4f} \\tmeanlatencyP2: {:.4f} \\tmeanlatencyP3: \\\n",
    "    {:.4f} \\tmeandatasize: {:.4f} \\tmeandatasizeP1: {:.4f} \\tmeandatasizeP2: {:.4f} \\tmeandatasizeP3: {:.4f} \\tmean_tran_latencyP1: {:.4f}  \\\n",
    "    \\tmean_tran_latencyP2: {:.4f}  \\tmean_tran_latencyP3: {:.4f}  \\tmean_comp_latencyP1: {:.4f}  \\\n",
    "    \\tmean_comp_latencyP2: {:.4f} \\tmean_comp_latencyP3: {:.4f}'.format(\n",
    "        i_episode, mean_score, np.mean(mean_tasks_P1[-window_size:]), np.mean(mean_tasks_P2[-window_size:]),\n",
    "        np.mean(mean_tasks_P3[-window_size:]), np.mean(mean_energy_P1[-window_size:]), np.mean(mean_energy_P2[-window_size:]),\n",
    "        np.mean(mean_energy_P3[-window_size:]), np.mean(mean_latency_P1[-window_size:]), np.mean(mean_latency_P2[-window_size:]),\n",
    "        np.mean(mean_latency_P3[-window_size:]), np.mean(mean_datasize[-window_size:]), np.mean(mean_datasize_P1[-window_size:]),\n",
    "        np.mean(mean_datasize_P2[-window_size:]), np.mean(mean_datasize_P3[-window_size:]), np.mean(mean_tran_latency_P1[-window_size:]),\n",
    "        np.mean(mean_tran_latency_P2[-window_size:]), np.mean(mean_tran_latency_P3[-window_size:]), np.mean(mean_comp_latency_P1[-window_size:]),\n",
    "        np.mean(mean_comp_latency_P2[-window_size:]), np.mean(mean_comp_latency_P3[-window_size:]))\n",
    ")\n",
    "                                                                                           \n",
    "\n",
    "            #print(len(task_done_P1),len(task_done_P2),len(task_done_P3))\n",
    "           # print(len(task_done_P1)+len(task_done_P2)+len(task_done_P3))\n",
    "            print('mean_task_till_this_window',np.mean(epi_task[-window_size:]), 'T1::',   np.mean(epi_task_P1[-window_size:]) ,'T2::' ,np.mean(epi_task_P2[-window_size:]) ,'T3::',np.mean(epi_task_P3[-window_size:]))\n",
    "            #print('mean_energy_till_this_window',np.mean(mean_energy[-window_size:]), 'T1::', np.mean(mean_energy_P1[-window_size:])  ,'T2::', np.mean(mean_energy_P2[-window_size:])  ,'T3::',np.mean(mean_energy_P3[-window_size:]))\n",
    "           # print('mean Latency till this window',np.mean(mean_latency[-window_size:]),'T1::', np.mean(mean_latency_P1[-window_size:]),'T1::', np.mean(mean_latency_P2[-window_size:]),'T1::', np.mean(mean_latency_P3[-window_size:]))\n",
    "           # print(type(mean_tasks_P1),type(epi_num_task_P1))\n",
    "            #print('mean_datasize', mean_datasize[-1])\n",
    "            print( 'task_select_P1', np.mean(epi_task_select_P1[-window_size:]), 'task_select_P2', np.mean(epi_task_select_P2[-window_size:]),'task_select_P3', np.mean(epi_task_select_P3[-window_size:]))\n",
    "            print('total_task_done_P1::', np.mean(epi_num_task_P1[-window_size:]),'total_task_done_P2::', np.mean(epi_num_task_P2[-window_size:]),'total_task_done_P3::',np.mean(epi_num_task_P3[-window_size:]))\n",
    "\n",
    "            #print(np.mean(reward_list_P1), np.mean(reward_list_P2), np.mean(reward_list_P3))\n",
    "            #print(min(reward_list_P1), min(reward_list_P2), min(reward_list_P3))\n",
    "           # print(max(reward_list_P1), max(reward_list_P2), max(reward_list_P3))\n",
    "            \n",
    "            #print(mean_tasks_P1)\n",
    "           # print(epi_task_P1)\n",
    "            print('mean_task_P1_ratio_by_bucket_51_10_100', np.mean(bucket_tasks_mean[(10,10+R_1)]['P1_task_done_mean'][-window_size:]),\n",
    "                  np.mean(bucket_tasks_mean[(10+R_1,10+2*R_1)]['P1_task_done_mean'][-window_size:]) , np.mean(bucket_tasks_mean[(10+2*R_1,10+3*R_1)]['P1_task_done_mean'][-window_size:])\n",
    "                  ,np.mean(bucket_tasks_mean[(10+3*R_1,10+4*R_1)]['P1_task_done_mean'][-window_size:]), np.mean(bucket_tasks_mean[(10+4*R_1,float('inf'))]['P1_task_done_mean'][-window_size:]))\n",
    "            print('mean_task_P2_ratio_by_bucket_51_10_100', np.mean(bucket_tasks_mean[(10,10+R_1)]['P2_task_done_mean'][-window_size:]),\n",
    "                  np.mean(bucket_tasks_mean[(10+R_1,10+2*R_1)]['P2_task_done_mean'][-window_size:]) , np.mean(bucket_tasks_mean[(10+2*R_1,10+3*R_1)]['P2_task_done_mean'][-window_size:])\n",
    "                  ,np.mean(bucket_tasks_mean[(10+3*R_1,10+4*R_1)]['P2_task_done_mean'][-window_size:]), np.mean(bucket_tasks_mean[(10+4*R_1,float('inf'))]['P2_task_done_mean'][-window_size:]))\n",
    "            print('mean_task_P3_ratio_by_bucket_51_10_100', np.mean(bucket_tasks_mean[(10,10+R_1)]['P3_task_done_mean'][-window_size:]),\n",
    "                  np.mean(bucket_tasks_mean[(10+R_1,10+2*R_1)]['P3_task_done_mean'][-window_size:]) , np.mean(bucket_tasks_mean[(10+2*R_1,10+3*R_1)]['P3_task_done_mean'][-window_size:])\n",
    "                  ,np.mean(bucket_tasks_mean[(10+3*R_1,10+4*R_1)]['P3_task_done_mean'][-window_size:]), np.mean(bucket_tasks_mean[(10+4*R_1,float('inf'))]['P3_task_done_mean'][-window_size:]))\n",
    "            print('mean_task_P1_select_by_bucket_51_10_100', np.mean(bucket_task_len[(10,10+R_1)]['P1_task_done_mean_len'][-window_size:]),\n",
    "                  np.mean(bucket_task_len[(10+R_1,10+2*R_1)]['P1_task_done_mean_len'][-window_size:]) , np.mean(bucket_task_len[(10+2*R_1,10+3*R_1)]['P1_task_done_mean_len'][-window_size:])\n",
    "                  ,np.mean(bucket_task_len[(10+3*R_1,10+4*R_1)]['P1_task_done_mean_len'][-window_size:]), np.mean(bucket_task_len[(10+4*R_1,float('inf'))]['P1_task_done_mean_len'][-window_size:]))\n",
    "            print('mean_task_P2_select_by_bucket_51_10_100', np.mean(bucket_task_len[(10,10+R_1)]['P2_task_done_mean_len'][-window_size:]),\n",
    "                  np.mean(bucket_task_len[(10+R_1,10+2*R_1)]['P2_task_done_mean_len'][-window_size:]) , np.mean(bucket_task_len[(10+2*R_1,10+3*R_1)]['P2_task_done_mean_len'][-window_size:])\n",
    "                  ,np.mean(bucket_task_len[(10+3*R_1,10+4*R_1)]['P2_task_done_mean_len'][-window_size:]), np.mean(bucket_task_len[(10+4*R_1,float('inf'))]['P2_task_done_mean_len'][-window_size:]))\n",
    "            print('mean_task_P3_select_by_bucket_51_10_100', np.mean(bucket_task_len[(10,10+R_1)]['P3_task_done_mean_len'][-window_size:]),\n",
    "                  np.mean(bucket_task_len[(10+R_1,10+2*R_1)]['P3_task_done_mean_len'][-window_size:]) , np.mean(bucket_task_len[(10+2*R_1,10+3*R_1)]['P3_task_done_mean_len'][-window_size:])\n",
    "                  ,np.mean(bucket_task_len[(10+3*R_1,10+4*R_1)]['P3_task_done_mean_len'][-window_size:]), np.mean(bucket_task_len[(10+4*R_1,float('inf'))]['P3_task_done_mean_len'][-window_size:]))\n",
    "\n",
    "            print('mean_task_P1_latency_by_bucket_51_10_100', np.mean(bucket_latency_mean[(10,10+R_1)]['P1_latency_mean'][-window_size:]),\n",
    "                  np.mean(bucket_latency_mean[(10+R_1,10+2*R_1)]['P1_latency_mean'][-window_size:]) , np.mean(bucket_latency_mean[(10+2*R_1,10+3*R_1)]['P1_latency_mean'][-window_size:])\n",
    "                  ,np.mean(bucket_latency_mean[(10+3*R_1,10+4*R_1)]['P1_latency_mean'][-window_size:]), np.mean(bucket_latency_mean[(10+4*R_1,float('inf'))]['P1_latency_mean'][-window_size:]))\n",
    "            print('mean_task_P2_latency_by_bucket_51_10_100', np.mean(bucket_latency_mean[(10,10+R_1)]['P2_latency_mean'][-window_size:]),\n",
    "                  np.mean(bucket_latency_mean[(10+R_1,10+2*R_1)]['P2_latency_mean'][-window_size:]) , np.mean(bucket_latency_mean[(10+2*R_1,10+3*R_1)]['P2_latency_mean'][-window_size:])\n",
    "                  ,np.mean(bucket_latency_mean[(10+3*R_1,10+4*R_1)]['P2_latency_mean'][-window_size:]), np.mean(bucket_latency_mean[(10+4*R_1,float('inf'))]['P2_latency_mean'][-window_size:]))\n",
    "            print('mean_task_P3_latency_by_bucket_51_10_100', np.mean(bucket_latency_mean[(10,10+R_1)]['P3_latency_mean'][-window_size:]),\n",
    "                  np.mean(bucket_latency_mean[(10+R_1,10+2*R_1)]['P3_latency_mean'][-window_size:]) , np.mean(bucket_latency_mean[(10+2*R_1,10+3*R_1)]['P3_latency_mean'][-window_size:])\n",
    "                  ,np.mean(bucket_latency_mean[(10+3*R_1,10+4*R_1)]['P3_latency_mean'][-window_size:]), np.mean(bucket_latency_mean[(10+4*R_1,float('inf'))]['P3_latency_mean'][-window_size:]))\n",
    "\n",
    "            print('mean_task_P1_energy_by_bucket_51_10_100', np.mean(bucket_energy_mean[(10,10+R_1)]['P1_energy_mean'][-window_size:]),\n",
    "                  np.mean(bucket_energy_mean[(10+R_1,10+2*R_1)]['P1_energy_mean'][-window_size:]) , np.mean(bucket_energy_mean[(10+2*R_1,10+3*R_1)]['P1_energy_mean'][-window_size:])\n",
    "                  ,np.mean(bucket_energy_mean[(10+3*R_1,10+4*R_1)]['P1_energy_mean'][-window_size:]), np.mean(bucket_energy_mean[(10+4*R_1,float('inf'))]['P1_energy_mean'][-window_size:]))\n",
    "            print('mean_task_P2_energy_by_bucket_51_10_100', np.mean(bucket_energy_mean[(10,10+R_1)]['P2_energy_mean'][-window_size:]),\n",
    "                  np.mean(bucket_energy_mean[(10+R_1,10+2*R_1)]['P2_energy_mean'][-window_size:]) , np.mean(bucket_energy_mean[(10+2*R_1,10+3*R_1)]['P2_energy_mean'][-window_size:])\n",
    "                  ,np.mean(bucket_energy_mean[(10+3*R_1,10+4*R_1)]['P2_energy_mean'][-window_size:]), np.mean(bucket_energy_mean[(10+4*R_1,float('inf'))]['P2_energy_mean'][-window_size:]))\n",
    "            print('mean_task_P3_energy_by_bucket_51_10_100', np.mean(bucket_energy_mean[(10,10+R_1)]['P3_energy_mean'][-window_size:]),\n",
    "                  np.mean(bucket_energy_mean[(10+R_1,10+2*R_1)]['P3_energy_mean'][-window_size:]) , np.mean(bucket_energy_mean[(10+2*R_1,10+3*R_1)]['P3_energy_mean'][-window_size:])\n",
    "                  ,np.mean(bucket_energy_mean[(10+3*R_1,10+4*R_1)]['P3_energy_mean'][-window_size:]), np.mean(bucket_energy_mean[(10+4*R_1,float('inf'))]['P3_energy_mean'][-window_size:]))\n",
    "\n",
    "            print('mean_data_P1_data_by_bucket_51_10_100', np.mean(bucket_data_mean[(10,10+R_1)]['P1_data_mean'][-window_size:]),\n",
    "                  np.mean(bucket_data_mean[(10+R_1,10+2*R_1)]['P1_data_mean'][-window_size:]) , np.mean(bucket_data_mean[(10+2*R_1,10+3*R_1)]['P1_data_mean'][-window_size:])\n",
    "                  ,np.mean(bucket_data_mean[(10+3*R_1,10+4*R_1)]['P1_data_mean'][-window_size:]), np.mean(bucket_data_mean[(10+4*R_1,float('inf'))]['P1_data_mean'][-window_size:]))\n",
    "            print('mean_data_P2_data_by_bucket_51_10_100', np.mean(bucket_data_mean[(10,10+R_1)]['P2_data_mean'][-window_size:]),\n",
    "                  np.mean(bucket_data_mean[(10+R_1,10+2*R_1)]['P2_data_mean'][-window_size:]) , np.mean(bucket_data_mean[(10+2*R_1,10+3*R_1)]['P2_data_mean'][-window_size:])\n",
    "                  ,np.mean(bucket_data_mean[(10+3*R_1,10+4*R_1)]['P2_data_mean'][-window_size:]), np.mean(bucket_data_mean[(10+4*R_1,float('inf'))]['P2_data_mean'][-window_size:]))\n",
    "            print('mean_data_P3_data_by_bucket_51_10_100', np.mean(bucket_data_mean[(10,10+R_1)]['P3_data_mean'][-window_size:]),\n",
    "                  np.mean(bucket_data_mean[(10+R_1,10+2*R_1)]['P3_data_mean'][-window_size:]) , np.mean(bucket_data_mean[(10+2*R_1,10+3*R_1)]['P3_data_mean'][-window_size:])\n",
    "                  ,np.mean(bucket_data_mean[(10+3*R_1,10+4*R_1)]['P3_data_mean'][-window_size:]), np.mean(bucket_data_mean[(10+4*R_1,float('inf'))]['P3_data_mean'][-window_size:]))\n",
    "\n",
    "\n",
    "            print('all_tran',np.mean(epi_all_trans_latency[-window_size:]), 'all_comp',np.mean(epi_all_comp_latency[-window_size:]))\n",
    "            \n",
    "            print('all_tran_P1',np.mean(epi_all_trans_latency_P1[-window_size:]),\\\n",
    "                  'all_tran_P2',np.mean(epi_all_trans_latency_P2[-window_size:]),\\\n",
    "                      'all_tran_P3',np.mean(epi_all_trans_latency_P3[-window_size:]))\n",
    "            \n",
    "            print('all_comp_P1',np.mean(epi_all_comp_latency_P1[-window_size:]),\\\n",
    "                  'all_comp_P2',np.mean(epi_all_comp_latency_P2[-window_size:]),\\\n",
    "                 'all_comp_P3',np.mean(epi_all_comp_latency_P3[-window_size:]))\n",
    "            \n",
    "            print('all_data_P1',np.mean(epi_all_datasize_P1[-window_size:]),\\\n",
    "                  'all_data_P2',np.mean(epi_all_datasize_P2[-window_size:]),\\\n",
    "                 'all_data_P3',np.mean(epi_all_datasize_P3[-window_size:]))\n",
    "            \n",
    "            \n",
    "            print('epi_comp_energy', np.mean(epi_comp_energy[-window_size:]))\n",
    "            print('epi_comp_freq', np.mean(epi_comp_frequency[-window_size:]))\n",
    "            print('epi_comp_latency', np.mean(epi_all_comp_latency[-window_size:]))\n",
    "            \n",
    "            if np.mean(scores_window)>=bench_score or i_episode % n_episodes==0:\n",
    "                print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.4f}'.format(i_episode, mean_score))\n",
    "                torch.save(agent.qnetwork_local.state_dict(), model_path)\n",
    "                break\n",
    "\n",
    "    return epi_scores,epi_task,epi_energy,epi_latency,epi_num_task, epi_task_P1,epi_task_P2,epi_task_P3,epi_latency_P1,epi_latency_P2,epi_latency_P3,epi_energy_P1,epi_energy_P2,epi_energy_P3,epi_num_task_P1,epi_num_task_P2,epi_num_task_P3, \\\n",
    "    epi_task_select_P1,epi_task_select_P2,epi_task_select_P3,mean_scores,mean_task,mean_energy,mean_latency,win_num_task, mean_tasks_P1,mean_tasks_P2,mean_tasks_P3, mean_energy_P1,\\\n",
    "                 mean_energy_P2, mean_energy_P3,mean_latency_P1,mean_latency_P2,mean_latency_P3,win_num_task_P1,win_num_task_P2,win_num_task_P3,epi_datasize,epi_datasize_P1,epi_datasize_P2,epi_datasize_P3,\\\n",
    "                 mean_datasize,mean_datasize_P1,mean_datasize_P2,mean_datasize_P3,epi_tran_latency_P1,epi_tran_latency_P2,\\\n",
    "                 epi_tran_latency_P3,epi_comp_latency_P1,epi_comp_latency_P2,epi_comp_latency_P3\n",
    "\n",
    "dqnb_epi_scores,dqnb_epi_task,dqnb_epi_energy,dqnb_epi_latency,dqnb_epi_num_task,dqnb_epi_task_P1,dqnb_epi_task_P2,dqnb_epi_task_P3,dqnb_epi_latency_P1,dqnb_epi_latency_P2,dqnb_epi_latency_P3,\\\n",
    "dqnb_epi_energy_P1,dqnb_epi_energy_P2,dqnb_epi_energy_P3,dqnb_epi_num_task_P1,dqnb_epi_num_task_P2,dqnb_epi_num_task_P3,dqnb_epi_task_select_P1,dqnb_epi_task_select_P2,dqnb_epi_task_select_P3,\\\n",
    "dqnb_mean_scores,dqnb_mean_task,dqnb_mean_energy,dqnb_mean_latency,dqnb_win_num_task, dqnb_mean_tasks_P1,dqnb_mean_tasks_P2,dqnb_mean_tasks_P3,\\\n",
    "dqnb_mean_energy_P1,dqnb_mean_energy_P2,dqnb_mean_energy_P3,dqnb_mean_latency_P1,dqnb_mean_latency_P2,dqnb_mean_latency_P3 ,dqnb_win_num_task_P1, dqnb_win_num_task_P2,dqnb_win_num_task_P3,\\\n",
    "dqnb_epi_datasize,dqnb_epi_datasize_P1,dqnb_epi_datasize_P2,dqnb_epi_datasize_P3,dqnb_mean_datasize,dqnb_mean_datasize_P1,dqnb_mean_datasize_P2,dqnb_mean_datasize_P3,\\\n",
    "dqnb_epi_tran_latency_P1,dqnb_epi_tran_latency_P2,dqnb_epi_tran_latency_P3,dqnb_epi_comp_latency_P1,dqnb_epi_comp_latency_P2,\\\n",
    "dqnb_epi_comp_latency_P3= dqnb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNcbaKBSXB1O"
   },
   "source": [
    "Episode 26\tAverage Score: 252.3442\tmeantaskP1: 0.2351\tmeantaskP2: 0.1988\tmeantaskP3: 0.1842\tmeanenergyP1:             0.0102\tmeanenergyP2: 0.0084\tmeanenergyP3: 0.0083\tmeanlatencyP1: 0.0309\tmeanlatencyP2: 0.0260\tmeanlatencyP3:0.0234\tmeanlatreward:0.0377\tmeanengreward:0.0272"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fv4vHu2WdTeh"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "# Combining data into a list of tuples\n",
    "data = zip(range(1, len(dqnb_epi_scores) + 1), dqnb_epi_scores,dqnb_epi_task,dqnb_epi_energy,dqnb_epi_latency,dqnb_epi_num_task,dqnb_epi_task_P1,dqnb_epi_task_P2,\n",
    "           dqnb_epi_task_P3,dqnb_epi_latency_P1,dqnb_epi_latency_P2,dqnb_epi_latency_P3,\n",
    "           dqnb_epi_energy_P1,dqnb_epi_energy_P2,dqnb_epi_energy_P3,dqnb_epi_num_task_P1,dqnb_epi_num_task_P2,dqnb_epi_num_task_P3,\n",
    "           dqnb_epi_datasize,dqnb_epi_datasize_P1,dqnb_epi_datasize_P2,dqnb_epi_datasize_P3,dqnb_epi_task_select_P1,dqnb_epi_task_select_P2,\\\n",
    "           dqnb_epi_task_select_P3,dqnb_epi_tran_latency_P1,dqnb_epi_tran_latency_P2,dqnb_epi_tran_latency_P3,\\\n",
    "                    dqnb_epi_comp_latency_P1,dqnb_epi_comp_latency_P2,dqnb_epi_comp_latency_P3)\n",
    "\n",
    "# Writing data to a CSV file\n",
    "with open(\"data_dqnb.csv\", mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Index\",\"dqnb_epi_scores\",\"dqnb_epi_task\",\"dqnb_epi_energy\",\"dqnb_epi_latency\",\"dqnb_epi_num_task\",\"dqnb_epi_task_P1\",\"dqnb_epi_task_P2\",\"dqnb_epi_task_P3\",\n",
    "                     \"dqnb_epi_latency_P1\",\"dqnb_epi_latency_P2\",\"dqnb_epi_latency_P3\",\"dqnb_epi_energy_P1\",\"dqnb_epi_energy_P2\",\"dqnb_epi_energy_P3\",\"dqnb_epi_num_task_P1\",\n",
    "                     \"dqnb_epi_num_task_P2\",\"dqnb_epi_num_task_P3\",\"dqnb_epi_datasize\",\"dqnb_epi_datasize_P1\",\"dqnb_epi_datasize_P2\"\n",
    "                     \"dqnb_epi_datasize_P3\",\"dqnb_epi_task_select_P1\",\"dqnb_epi_task_select_P2\",\"dqnb_epi_task_select_P3\",\\\n",
    "                    \"dqnb_epi_tran_latency_P1\",\"dqnb_epi_tran_latency_P2\",\"dqnb_epi_tran_latency_P3\",\\\n",
    "                    \"dqnb_epi_comp_latency_P1\",\"dqnb_epi_comp_latency_P2\",\"dqnb_epi_comp_latency_P3\"])  # Writing header\n",
    "    writer.writerows(data)  # Writing data rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OPOR-vhHGnKp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E0IPp62mG4OL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBbu3Rgm1yrC"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
